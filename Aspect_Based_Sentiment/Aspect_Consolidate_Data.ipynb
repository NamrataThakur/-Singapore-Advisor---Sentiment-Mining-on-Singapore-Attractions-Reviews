{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9325,
     "status": "ok",
     "timestamp": 1682755100826,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "IglyHItD8EUA",
    "outputId": "cc510497-880f-4e63-f36b-404ff34834f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 348.2/981.5 kB 7.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- -- 911.4/981.5 kB 9.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 8.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\ray\\anaconda3\\envs\\cv\\lib\\site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993253 sha256=3edd36ce5a9549c33da8e0095baf536340570431c05ccddc28f736ec2a573d4a\n",
      "  Stored in directory: c:\\users\\ray\\appdata\\local\\pip\\cache\\wheels\\13\\c7\\b0\\79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     ---------------------------------------- 0.0/289.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 289.9/289.9 kB 9.0 MB/s eta 0:00:00\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp38-cp38-win_amd64.whl (39 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect\n",
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "ok",
     "timestamp": 1682755102369,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "fJTgnuV0LwLH",
    "outputId": "94d708c2-bd78-425f-db4c-fd5fe07ecec1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ray\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ray\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split sentences\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "#text preprocess file\n",
    "import unicodedata\n",
    "import contractions         \n",
    "import re\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b4x_1fxzcdT"
   },
   "source": [
    "# Load review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1683292125530,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "OpjvCrYnyzQT",
    "outputId": "645ca8d8-0d19-4f8d-8bb5-f4c2853e1ef1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_sentence-multiple-aspect-Arab Street_updated.csv',\n",
       " 'new_sentence-multiple-aspect-ArtScience Museum at Marina Bay Sands_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Buddha Tooth Relic Temple and Museum_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Clarke Quay_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Gardens by the Bay_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Jurong Bird Park_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Maxwell Food Centre_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Merlion Park_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Mustafa Centre_updated.csv',\n",
       " 'new_sentence-multiple-aspect-National Museum of Singapore_updated.csv',\n",
       " 'new_sentence-multiple-aspect-National Orchid Garden_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Night Safari_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Orchard Road_updated.csv',\n",
       " 'new_sentence-multiple-aspect-River Wonders_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Sands Skypark Observation Deck_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Singapore Botanic Gardens_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Singapore Flyer_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Singapore Mass Rapid Transit SMRT_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Singapore River_updated.csv',\n",
       " 'new_sentence-multiple-aspect-Singapore Zoo_updated.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all review data\n",
    "import os\n",
    "folder_path = './SentenceMultiple'\n",
    "file_names = os.listdir(folder_path)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of amusements: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                               Arab Street\n",
       "1      Buddha Tooth Relic Temple and Museum\n",
       "2                               Clarke Quay\n",
       "3                        Gardens by the Bay\n",
       "4                          Jurong Bird Park\n",
       "5     ArtScience Museum at Marina Bay Sands\n",
       "6                       Maxwell Food Centre\n",
       "7                              Merlion Park\n",
       "8                            Mustafa Centre\n",
       "9              National Museum of Singapore\n",
       "10                   National Orchid Garden\n",
       "11                             Night Safari\n",
       "12                             Orchard Road\n",
       "13                            River Wonders\n",
       "14           Sands Skypark Observation Deck\n",
       "15                          Singapore Flyer\n",
       "16                          Singapore River\n",
       "17                            Singapore Zoo\n",
       "18                Singapore Botanic Gardens\n",
       "19        Singapore Mass Rapid Transit SMRT\n",
       "Name: Place, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load all places\n",
    "amusements = pd.read_excel('./20_places.xlsx')['Place']\n",
    "\n",
    "print(f\"Number of amusements: {len(amusements)}\")\n",
    "amusements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crOfUCTWsVSS"
   },
   "source": [
    "# Process Review Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Yrvels-19WaC"
   },
   "outputs": [],
   "source": [
    "def split_sentences(text):\n",
    "    return re.split(r'[.!?;]', text)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    # Define a list of ASCII characters\n",
    "    ascii_letters = string.ascii_letters + string.digits + string.punctuation + ' '\n",
    "    \n",
    "    # Filter out non-ASCII characters\n",
    "    ascii_text = ''.join(filter(lambda x: x in ascii_letters, text))\n",
    "    \n",
    "    return ascii_text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Define the pattern to match special characters\n",
    "    pattern = r'[^a-zA-Z\\s]'\n",
    "    # Remove the special characters\n",
    "    filtered_text = re.sub(pattern, '', text)\n",
    "    return filtered_text\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    # Define a list of stopwords for the English language\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    # Remove the stopwords\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Join the filtered words back into a text string\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "\n",
    "def pos_tagger(review):\n",
    "    #print(review)\n",
    "    lemma_text = []\n",
    "    text = nltk.word_tokenize(review)\n",
    "    pos_text = nltk.pos_tag(text)\n",
    "    for word, tag in pos_text:\n",
    "        if tag[0] in ['N', 'V', 'J', 'R']:\n",
    "            if tag in ('NN','NNPS','NNP','NNS','RB', 'RBR', 'RBS','VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ','ADV'):\n",
    "                lemma_text.append(wnl.lemmatize(word,pos= tag[0].lower()))\n",
    "            elif tag in ('JJR', 'JJS','JJ'):\n",
    "                lemma_text.append(wnl.lemmatize(word, pos = 'a'))\n",
    "            else:\n",
    "                lemma_text.append(word)\n",
    "    return lemma_text\n",
    "\n",
    "def lemmatization_tagger(text):\n",
    "   \n",
    "    clean_text = pos_tagger(text)\n",
    "    lemma_text = ' '.join(clean_text)\n",
    "    return lemma_text\n",
    "\n",
    "def detect_lang(text):\n",
    "    try:\n",
    "        lang = detect(x)\n",
    "    except:\n",
    "        lang = 'others'\n",
    "    return lang\n",
    "\n",
    "def preprocess(text):\n",
    "    text = replace_contractions(text)\n",
    "    text = remove_non_ascii(text)\n",
    "    text = text.lower()\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = lemmatization_tagger(text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBhBJg5gnpeQ"
   },
   "source": [
    "# Find Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 632,
     "status": "ok",
     "timestamp": 1684396654209,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "mFDj4bAUntwT"
   },
   "outputs": [],
   "source": [
    "# load aspect terms\n",
    "\n",
    "def get_aspects_df(df, place):\n",
    "  aspect_df = df.loc[df['Place'] == place][['Aspect', 'Aspect Terms']]\n",
    "  aspect_term_data =  [data.replace('\\'', '').replace('[', '').replace(']', '').split(',') for data in aspect_df['Aspect Terms']]\n",
    "  # pattern = r\"['](.*?)[']\"\n",
    "  # aspect_term_data = [re.findall(pattern, data) for data in  aspect_df['Aspect Terms']]\n",
    "  aspect_df['Aspect Terms'] = aspect_term_data\n",
    "  return aspect_df\n",
    "\n",
    "aspect_term_df = pd.read_excel('./aspects.xlsx')[['Place', 'Aspect', 'Aspect Terms']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1684396348805,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "lecUvy_k5nue",
    "outputId": "9a49b4ea-317c-4d9a-e8a6-9d4d874bf9fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arab Street',\n",
       " 'ArtScience Museum at Marina Bay Sands',\n",
       " 'Buddha Tooth Relic Temple and Museum',\n",
       " 'Clarke Quay',\n",
       " 'Gardens by the Bay',\n",
       " 'Jurong Bird Park',\n",
       " 'Maxwell Food Centre',\n",
       " 'Merlion Park',\n",
       " 'Mustafa Centre',\n",
       " 'National Museum of Singapore',\n",
       " 'National Orchid Garden',\n",
       " 'Orchard Road',\n",
       " 'River Wonders',\n",
       " 'Sands Skypark Observation Deck',\n",
       " 'Singapore Botanic Gardens',\n",
       " 'Singapore Flyer',\n",
       " 'Singapore Mass Rapid Transit SMRT',\n",
       " 'Singapore River',\n",
       " 'Singapore Zoo'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(aspect_term_df['Place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 547,
     "status": "ok",
     "timestamp": 1684396660222,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "XFhmsD0yjlCH",
    "outputId": "a8441b3c-3865-4c8c-f1a2-e195a0fc2667"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Aspect Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Culture &amp; Religion</td>\n",
       "      <td>[street,  lane,  muslim,  soak,  sultan,  neig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shopping</td>\n",
       "      <td>[shisha,  textile,  ware,  silk,  shop,  rug, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Food</td>\n",
       "      <td>[restuarant,  pub,  dining,  food,  cuisine,  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Aspect                                       Aspect Terms\n",
       "0  Culture & Religion  [street,  lane,  muslim,  soak,  sultan,  neig...\n",
       "1            Shopping  [shisha,  textile,  ware,  silk,  shop,  rug, ...\n",
       "2                Food  [restuarant,  pub,  dining,  food,  cuisine,  ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_aspects_df(aspect_term_df, \"Arab Street\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 556,
     "status": "ok",
     "timestamp": 1684396664268,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "R2Mu9J_SsYoo",
    "outputId": "f3dbb943-909d-4d62-e194-3462b3115234"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Transportation & Accessibility', 'Cleanliness', 'Value for Money'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load common aspects\n",
    "common_aspects = {}\n",
    "common_aspects[\"Transportation & Accessibility\"] =  ['destination', 'bus', 'map', 'transit', 'metro', 'transport', 'transportation', 'network', 'credit', 'station', 'smrt','train', 'pass', 'subway', 'route', 'journey', 'passenger', 'direction']\n",
    "common_aspects[\"Cleanliness\"] = ['sanitation', 'hygiene', 'tidiness', 'orderliness', 'neatness', 'clean', 'spotless', 'dirt', 'dust', 'litter', 'rubbish', 'stain', 'mess', 'trash', 'grime', 'smudge', 'spill']\n",
    "common_aspects[\"Value for Money\"] = ['price', 'cost', 'fee', 'discount', 'rate', 'value', 'expensive', 'cheap', 'affordable', 'budget', 'deal', 'discount', 'savings', 'economy', 'bargain', 'investment', 'worth']\n",
    "common_aspects.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 913,
     "status": "ok",
     "timestamp": 1684396667957,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "614qohDCyy6p"
   },
   "outputs": [],
   "source": [
    "def find_aspect(word, aspects, aspect_terms):\n",
    "    for aspect, terms in zip(aspects, aspect_terms):\n",
    "        for term in terms:\n",
    "            if word == term:\n",
    "                return aspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16133,
     "status": "ok",
     "timestamp": 1684396686335,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "HUpJUJmqski5",
    "outputId": "5859be8a-ed88-406f-c068-40fa9ee39628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processing Arab Street\n",
      "2 processing Buddha Tooth Relic Temple and Museum\n",
      "3 processing Clarke Quay\n",
      "4 processing Gardens by the Bay\n",
      "5 processing Jurong Bird Park\n",
      "6 processing ArtScience Museum at Marina Bay Sands\n",
      "7 processing Maxwell Food Centre\n",
      "8 processing Merlion Park\n",
      "9 processing Mustafa Centre\n",
      "10 processing National Museum of Singapore\n",
      "11 processing National Orchid Garden\n",
      "12 processing Night Safari\n",
      "13 processing Orchard Road\n",
      "14 processing River Wonders\n",
      "15 processing Sands Skypark Observation Deck\n",
      "16 processing Singapore Flyer\n",
      "17 processing Singapore River\n",
      "18 processing Singapore Zoo\n",
      "19 processing Singapore Botanic Gardens\n",
      "20 processing Singapore Mass Rapid Transit SMRT\n"
     ]
    }
   ],
   "source": [
    "amusement_n = 0 \n",
    "for amusement in amusements:\n",
    "    amusement_n +=1\n",
    "    input_file = f'./SentenceMultiple/new_sentence-multiple-aspect-{amusement}_updated.csv'\n",
    "    output_file = f'./SentenceConsolidated/consolidated-{amusement}.csv'\n",
    "\n",
    "\n",
    "    print(f'{amusement_n} processing {amusement}')\n",
    "    df = get_aspects_df(aspect_term_df, amusement)\n",
    "    aspects = list(df['Aspect'])\n",
    "    aspect_terms = list(df[\"Aspect Terms\"])\n",
    "\n",
    "    # add common aspects\n",
    "    for common_aspect in common_aspects.keys():\n",
    "        if len(aspects) < 5 and common_aspect not in aspects:\n",
    "\n",
    "            aspects.append(common_aspect)\n",
    "            aspect_terms.append(common_aspects[common_aspect])\n",
    "\n",
    "   \n",
    "    with open(input_file, 'r', encoding='utf-8', errors='replace') as infile, open(output_file, 'w', encoding='utf-8', errors='replace', newline='') as outfile:\n",
    "#         fieldnames_old = reader.fieldnames\n",
    "#         print(fieldnames_old)\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = ['place', 'reviewer', 'reviewer_location', 'reviewer_contributions', 'review_rating', 'review_type', 'review_date', 'review_title', 'review_text', 'review_preprocessed']\n",
    "        new_fieldnames = fieldnames + ['aspect_term', 'aspect', 'sentiment']\n",
    "\n",
    "        writer = csv.DictWriter(outfile, fieldnames=new_fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            for i in range(1, 6):\n",
    "                t_header = f'target_{i}'\n",
    "                if t_header in row.keys():\n",
    "                    aspect_word = row[t_header]\n",
    "                    if aspect_word:\n",
    "                        prepprocessed_word = preprocess(aspect_word).split(\" \")[0]\n",
    "                        aspect_name = find_aspect(prepprocessed_word, aspects, aspect_terms)\n",
    "                        if aspect_name:\n",
    "                            new_row = {key: row[key] for key in fieldnames}\n",
    "                            new_row['aspect_term'] = prepprocessed_word\n",
    "                            new_row['aspect'] = aspect_name\n",
    "                            new_row['sentiment'] = row[f'sentiment_{i}']\n",
    "                            writer.writerow(new_row)\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jnz4oPVL85QA"
   },
   "source": [
    "# Append Labeled data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 驱动器 C 中的卷是 Windows-SSD\n",
      " 卷的序列号是 5EC1-52B9\n",
      "\n",
      " C:\\Users\\Ray\\Desktop\\review 的目录\n",
      "\n",
      "2023/05/20  01:25    <DIR>          .\n",
      "2023/05/19  16:50    <DIR>          ..\n",
      "2023/05/19  23:36    <DIR>          .ipynb_checkpoints\n",
      "2023/05/18  19:20             8,946 20_places.xlsx\n",
      "2023/05/20  01:22           201,215 ABSA_fine_tune.ipynb\n",
      "2023/05/19  22:27            19,749 aspects.xlsx\n",
      "2023/05/20  01:25            54,238 Aspect_Consolidate_Data.ipynb\n",
      "2023/05/19  01:19           158,230 aspect_label.ipynb\n",
      "2023/05/20  00:15    <DIR>          Model\n",
      "2023/05/19  23:02    <DIR>          SentenceConsolidated\n",
      "2023/05/19  23:44    <DIR>          SentenceInfer\n",
      "2023/05/20  01:22    <DIR>          SentenceInferData\n",
      "2023/05/19  22:06    <DIR>          SentenceMultiple\n",
      "2023/05/19  23:41    <DIR>          SentenceMultipleLabel\n",
      "2023/05/18  21:32    <DIR>          SentenceTrainData\n",
      "2023/05/19  21:31             5,500 Untitled.ipynb\n",
      "               6 个文件        447,878 字节\n",
      "              10 个目录 304,289,439,744 可用字节\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 958814,
     "status": "ok",
     "timestamp": 1682759386333,
     "user": {
      "displayName": "Hui OUYANG",
      "userId": "00425198284664068842"
     },
     "user_tz": -480
    },
    "id": "m8uQDJETzl-G",
    "outputId": "af4bdfba-2b50-4a49-c7ff-ef11b9e8e276",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 processing Arab Street\n",
      "total number of sentence: 1144\n",
      "2 processing Buddha Tooth Relic Temple and Museum\n",
      "total number of sentence: 1693\n",
      "3 processing Clarke Quay\n",
      "total number of sentence: 1797\n",
      "4 processing Gardens by the Bay\n",
      "total number of sentence: 16278\n",
      "5 processing Jurong Bird Park\n",
      "total number of sentence: 1200\n",
      "6 processing ArtScience Museum at Marina Bay Sands\n",
      "total number of sentence: 1222\n",
      "7 processing Maxwell Food Centre\n",
      "total number of sentence: 3752\n",
      "8 processing Merlion Park\n",
      "total number of sentence: 1491\n",
      "9 processing Mustafa Centre\n",
      "total number of sentence: 573\n",
      "10 processing National Museum of Singapore\n",
      "total number of sentence: 2592\n",
      "11 processing National Orchid Garden\n",
      "total number of sentence: 821\n",
      "12 processing Night Safari\n",
      "total number of sentence: 2292\n",
      "13 processing Orchard Road\n",
      "total number of sentence: 3213\n",
      "14 processing River Wonders\n",
      "total number of sentence: 1083\n",
      "15 processing Sands Skypark Observation Deck\n",
      "total number of sentence: 2755\n",
      "16 processing Singapore Flyer\n",
      "total number of sentence: 5781\n",
      "17 processing Singapore River\n",
      "total number of sentence: 501\n",
      "18 processing Singapore Zoo\n",
      "total number of sentence: 13863\n",
      "19 processing Singapore Botanic Gardens\n",
      "total number of sentence: 1007\n",
      "20 processing Singapore Mass Rapid Transit SMRT\n",
      "total number of sentence: 6435\n"
     ]
    }
   ],
   "source": [
    "# Load data and set labels\n",
    "cols = ['place', 'reviewer', 'reviewer_location', 'reviewer_contributions',\n",
    "       'review_rating', 'review_type', 'review_date', 'review_title',\n",
    "       'review_text', 'review_preprocessed', 'aspect_term', 'aspect',\n",
    "       ]\n",
    "\n",
    "\n",
    "amusement_n = 0 \n",
    "for amusement in amusements:\n",
    "    amusement_n +=1\n",
    "    print(f'{amusement_n} processing {amusement}')\n",
    "\n",
    "    output_file = f'./SentenceConsolidated/consolidated-{amusement}.csv'\n",
    "    \n",
    "    multiple_file = f'./SentenceMultipleLabel/consolidated-{amusement}.csv'\n",
    "    labeled_file = f'./SentenceTrainData/train-label-{amusement}.csv'\n",
    "    inferred_file = f'./SentenceInferData/infer-label-{amusement}.csv'\n",
    "    \n",
    "    data_df =  pd.read_csv(multiple_file)\n",
    "    df = pd.read_csv(labeled_file)\n",
    "    labeled_df = df[cols].copy()\n",
    "#     labeled_df['sentiment'] = df['label']\n",
    "    labeled_df.loc[:,'sentiment'] = df['label']\n",
    "    \n",
    "    inferred_df =  pd.read_csv(inferred_file)[cols+['sentiment']]\n",
    "    \n",
    "    data_df = pd.concat([data_df, labeled_df, inferred_df], ignore_index=True)\n",
    "    aspects =  [data.replace('\\'', '').replace('[', '').replace(']', '') for data in data_df['aspect']]\n",
    "    data_df['aspect'] = aspects\n",
    "    print(f'total number of sentence: {data_df.shape[0]}')\n",
    "    data_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74238, 6244)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sentences = [\n",
    "    1176, 1734, 2330, 18016, 1206, 1255, 3823, 1546, 595, 2690,\n",
    "    825, 2332, 3405, 1100, 2798, 5824, 510, 15422, 1010, 6641\n",
    "]\n",
    "\n",
    "sentences_with_multiple_aspects = [\n",
    "    41, 60, 645, 2222, 7, 41, 691, 62, 24, 118,\n",
    "    4, 74, 253, 22, 45, 61, 12, 1594, 3, 265\n",
    "]\n",
    "sum(total_sentences), sum(sentences_with_multiple_aspects)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNiWWrEgb6uQZxIwWrIZuVO",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
